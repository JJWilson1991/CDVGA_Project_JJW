---
title: "ExploratoryAnalysis"
author: "JJWilson"
date: "11/10/2019"
output: html_document
---

#Load the required libraries 



```{r}

library(readxl)

library(dplyr)

library(tidyverse)

library(forcats)

library(ggthemes)

library(plotly)

library(knitr)

library(naniar)

library(ggplot2)

library(maps)

library(ggmap)

library(maptools)

library(sf)

```



#Load the processed data from the RDS



```{r}

CDV_Clean <-   readRDS("../../data/processed_data/CDVprocesseddata.rds")



glimpse(CDV_Clean)

```
```{r}
#The data we are most interested in is the Species data over time and the area data over time so we'll begin by exploring these

Species_freq <- ggplot(CDV_Clean, aes(Species, fill=Species, color=Species)) +geom_bar()  +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8), legend.position = "none")

Species_freq

ggsave(filename = "../../results/Species_freq.png",plot = Species_freq)
#we can see that Raccoon and Gray fox are hugely over represented compared to the others so we should focus our analysis on these two species
```
```{r}
library(RColorBrewer)
#are there any obvious differneces in species by state?
Species_by_state <- ggplot(CDV_Clean, aes(Species, fill=Species, color=Species)) +
geom_bar()   + 
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8), legend.position = "none") +facet_wrap( ~ State)

Species_by_state

ggsave(filename = "../../results/Species_by_state.png",plot = Species_by_state)
```
```{r}
raccoongrayfox <- subset(CDV_Clean, Species %in% c("Raccoon", "Gray Fox"))
SPperYr <- with(raccoongrayfox, table(Species, CollectionYear))
RacFox_year<- ggplot(as.data.frame(SPperYr), aes(CollectionYear, Freq, fill = Species, color = Species, group = Species))  + 
  geom_point() +geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))

RacFox_year

ggsave(filename = "../../results/RacFox_year.png",plot = RacFox_year)
#looking overtime there appears to be some patterns in the number of postive cases, with definite peaks possibly relating to epizootics of CDV
#It also looks like there may be a delay in the Gray Fox peak following the peak in Raccoons, this is worth looking into
#Would be good to run a time series analysis on this data to see if peaks in raccoon cases are predicitive of grayfox peaks
```

```{r}
State_freq <- ggplot(CDV_Clean, aes(State, fill=State, color=State)) +geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8), legend.position = "none")

State_freq

ggsave(filename = "../../results/State_freq.png",plot = State_freq)
#GA is the majority of cases which is understandable as SCWDS is in Athens, but there are enough cases in other south eastern states to possibly gain some insight
#but i think the bulk of the mapping should focus on the county data within GA

```
```{r}
State_wrap_species <- ggplot(CDV_Clean, aes(State, fill=State, color=State)) +geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8), legend.position = "none") + 
  facet_wrap(~Species)

State_wrap_species

ggsave(filename = "../../results/State_wrap_species.png",plot = State_wrap_species)
```
```{r}
StperYr <- with(CDV_Clean, table(State, CollectionYear))
point_casesyear_state <- ggplot(as.data.frame(StperYr), aes(CollectionYear, Freq, fill = State, color = State, group = State))  + 
  geom_point() +
  geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))

point_casesyear_state

ggsave(filename = "../../results/point_casesyear_state.png",plot = point_casesyear_state)
#the cases per state over time seem to follow a similar pattern but it may be worth plotting species cases per state and mapping location, maybe certain species are more affected in different states at different times?


```

```{r}
#we're basically repaeting the above but including species to see if this varies by state
SPYRST <- with(CDV_Clean, table(State, CollectionYear, Species))
point_casesyear_state_wrap_species <-    ggplot(as.data.frame(SPYRST), aes(CollectionYear, Freq, fill = State, color = State, group = State))  +
  geom_point() +geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6)) + 
  facet_wrap( ~ Species)

point_casesyear_state_wrap_species

ggsave(filename = "../../results/point_casesyear_state_wrap_species.png",plot = point_casesyear_state_wrap_species)

#the number of cases in species other than gray fox and raccon is so low its probably better to just focus on those 2 species gping forward
```
```{r}
SP_Fr_Yr_WrapState <- ggplot(as.data.frame(SPYRST), aes(CollectionYear, Freq, fill = Species, color = Species, group = Species))  + 
  geom_point() +
  geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6)) + 
  facet_wrap( ~ State)

SP_Fr_Yr_WrapState

ggsave(filename = "../../results/SP_Fr_Yr_WrapState.png",plot = SP_Fr_Yr_WrapState)


#there does seem to be a little bit of variation here between state over time, maybe theyre epidemics spreading from a nidus? worth investigating in maps
```

```{r}
#lets take a look if there is any obvious variation in ages affected between raccoon and  gray fox
RGFAge <- subset(CDV_Clean, Species %in% c("Raccoon", "Gray Fox") & Age %in% c("Adult", "Subadult", "Juvenile"))
RacFox_Age <- ggplot(RGFAge, aes(Species, color=Species)) +geom_bar(aes(fill = Age))

RacFox_Age

ggsave(filename = "../../results/RacFox_Age.png",plot = RacFox_Age)
#there may be something here, worth looking at statistically going forward
```
```{r}
tib <-  as.data.frame(with(RGFAge, table(Age, Species)))
          
TibageSP <- tib %>% spread(key = Age, value = Freq)
TibageSPd <- TibageSP%>% dplyr::mutate(Total = (Adult + Subadult +Juvenile))

tb<- TibageSPd %>% mutate_at(vars(Adult, Juvenile, Subadult), funs(./Total))
tb

```

```{r}
#i forgot to plot age by species wrap.. its pretty uninteresting

ggplot(CDV_Clean, aes(Sex)) + geom_bar() +facet_wrap(~Species)
```
```{r}
RGFSex <- subset(CDV_Clean, Species %in% c("Raccoon", "Gray Fox") & Sex %in% c("Male", "Female"))

RacFox_Sex <- ggplot(RGFSex, aes(Sex)) + 
  geom_bar() +
  facet_wrap(~Species)
RacFox_Sex

ggsave(filename = "../../results/RacFox_Sex.png",plot = RacFox_Sex)
# There may be a significant difference in the M/F distribution for gray foxes, worth analsing
```
```{r}
#as there seemed to potnetially be some difference in gender distribution, lets look at gender difference over time between raccoon and gray fox
#ceratinly in the earlier time frame there seems to be more female gray foxes affected, has this got a role in why there were many fewer cases after this as population has fallen?
RGFSEXYEAR<- with(RGFSex, table(Species, CollectionYear, Sex))
Gender_year <- ggplot(as.data.frame(RGFSEXYEAR), aes(CollectionYear, Freq, fill = Sex, color = Sex, group = Sex))  + 
  geom_point() +geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6)) + facet_wrap(~Species)

Gender_year
```

```{r}
#If we quickly plot a histogram using latitude it suggest there are some geographic differences in distribution 
LAT <- left_join(GAC, CDVGA2, by= "County") %>% subset( Species %in% c("Raccoon", "Gray Fox"))

ggplot(LAT, aes(lat)) + geom_histogram()  +
 facet_wrap(~Species)

#This is interesting, there are two disticnt latitudinal peaks for raccoons but only 1 for gray foxes
```







```{r}
#exper
library('tidyr')
library('dplyr')
library('forcats')
library('ggplot2')
library('corrplot') #to make a correlation plot. You can use other options/packages.
library('visdat') #for missing data visualization
library('caret') #for model fitting
library('earth')
glimpse(allmod)
 remove_NA <- allmod[!is.na(allmod$Sex), ]  
 remo2 <- remove_NA[!is.na(remove_NA$Species), ]
  
  remo3 <- remo2[!is.na(remo2$Age), ]
   remo4 <- remo3[!is.na(remo3$lat), ]
   remo5 <- remo4[!is.na(remo4$long), ]
 
   vis_miss(remo5)
```   
```{r}  
   d<- remo5
set.seed(123)
trainset <- caret::createDataPartition(y = d$lat, p = 0.7, list = FALSE)
data_train = d[trainset,] 
#extract observations/rows for training, assign to new variable
data_test = d[-trainset,] 
#do the same for the test set
```

:::note
Since the above code involves drawing samples, and we want to do that reproducible, we also set a random number seed with `set.seed()`. With that, each time we perform this sampling, it will be the same, unless we change the seed. If nothing about the code changes, setting the seed once at the beginning is enough. If you want to be extra sure, it is a good idea to set the seed at the beginning of every code chunk that involves random numbers (i.e., sampling or some other stochastic/random procedure). We do that here.
:::

## A null model

Now let's begin with the model fitting. We'll start by looking at a _null model_, which is just the mean of the data. This is, of course, a stupid "model" but provides some baseline for performance.

```{r clean-na}
#write code that computes the RMSE for a null model, which is just the mean of the outcome
#remember that from now on until the end, everything happens with the training data

mean(data_train$lat)
```


## Single predictor models

Now we'll fit the outcome to each predictor one at a time. To evaluate our model performance, we will use cross-validation and the caret package. Note that we just fit a linear model. `caret` itself is not a model. Instead, it provides an interface that allows easy access to many different models and has functions to do a lot of steps quickly - as you will see below. Most of the time, you can do all our work through the `caret` (or `mlr`) workflow. The problem is that because `caret` calls another package/function, sometimes things are not as clear, especially when you get an error message. So occasionally, if you know you want to use a specific model and want more control over things, you might want to not use `caret` and instead go straight to the model function (e.g. `lm` or `glm` or...). We've done a bit of that before, for the remainder of the class we'll mostly access underlying functions through `caret`.

```{r}
#put outcome ie. fracinf first
data_train <- data_train[, c(1,2,3,4,5,6,7)]
#There is probably a nicer tidyverse way of doing this. I just couldn't think of it, so did it this way.
set.seed(1111) 
#makes each code block reproducible
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #setting CV method for caret
Npred <- ncol(data_train)-1 
# number of predictors
resultmat <- data.frame(Variable = names(data_train)[-1], RMSE = rep(0,Npred)) 
#store values for RMSE for each variable
for (n in 2:ncol(data_train)) 
  #loop over each predictor. For this to work, outcome must be in 1st column
{
  fit1 <- train( as.formula(paste("lat ~",names(data_train)[n])) , data = data_train, method = "lm", trControl = fitControl) 
 resultmat[n-1,2]= fit1$results$RMSE  
}
print(resultmat)
```

This analysis shows 2 things that might need closer inspections. We get some error/warning messages, and most RMSE of the single-predictor models are not better than the null model. Usually, this is cause for more careful checking until you fully understand what is going on. But for this exercise, let's blindly press on!


## Multi-predictor models

Now let's perform fitting with multiple predictors. Use the same setup as the code above to fit the outcome to all predictors at the same time. Do that for 3 different models: linear (`lm`), regression splines (`earth`), K nearest neighbor (`knn`). You might have to install/load some extra R packages for that. If that's the case, `caret` will tell you.

```{r}
set.seed(1111) #makes each code block reproducible
#write code that uses the train function in caret to fit the outcome to all predictors using the 3 methods specified.
#report the RMSE for each method. Note that knn and earth perform some model tuning (we'll discuss this soon) and report multiple RMSE. Use the lowest value.

lm_fit <- train(lat~., data=data_train,method="lm", trControl = fitControl)

lm_fit

```
```{r}
set.seed(1111)

earth_fit_1 <- train(lat~., data=data_train,method="earth", trControl = fitControl)

earth_fit_1 


```

```{r}
set.seed(1111) 

knn_fit <- train(lat~., data=data_train,method="knn", trControl = fitControl)

knn_fit 

#lowest RMSE = 10.61300
```
So we find that some of these models do better than the null model and the single-predictor ones. KNN seems the best of those 3. Next, we want to see if pre-processing our data a bit more might lead to even better results.


## Multi-predictor models with pre-processing

Above, we fit outcome and predictors without doing anything to them. Let's see if some further processing improves the performance of our multi-predictor models.

First, we look at near-zero variance predictors. Those are predictors that have very little variation. For instance, for a categorical predictor, if 99% of the values are a single category, it is likely not a useful predictor. A similar idea holds for continuous predictors. If they have very little spread, they might likely not contribute much 'signal' to our fitting and instead mainly contain noise. Some models, such as trees, which we'll cover soon, can ignore useless predictors and just remove them. Other models, e.g., linear models, are generally performing better if we remove such useless predictors.

Note that in general, one should apply all these processing steps to the training data only. Otherwise, you would use information from the test set to decide on data manipulations for all data (called data leakage). It is a bit hard to say when to make the train/test split. Above, we did a good bit of cleaning on the full dataset before we split. One could argue that one should split right at the start, then do the cleaning. However, this doesn't work for certain procedures (e.g., removing observations with NA). 

```{r}
#write code using the caret function `nearZeroVar` to look at potential uninformative predictors. Set saveMetrics to TRUE. Look at the results 
nearZeroVar(data_train, saveMetrics = TRUE)

```

You'll see that several variables are flagged as having near-zero variance. Look for instance at `Deaths`, you'll see that almost all outbreaks have zero deaths. It is a judgment call if we should remove all those flagged as near-zero-variance or not. For this exercise, we will.


```{r}
#write code that removes all variables with near zero variance from the data 

#nearZero <- nearZeroVar(data_train)

#trainDrop <- data_train[, -nearZero]

#glimpse(trainDrop)
```

You should be left with 13 variables (including the outcome).


Next, we noticed during our exploratory analysis that it might be useful to center and scale predictors. So let's do that now. With caret, one can do that by providing the `preProc` setting inside the `train` function. Set it to center and scale the data, then run the 3 models from above again.


```{r processed-fit}
#write code that repeats the multi-predictor fits from above, but this time applies centering and scaling of variables.
#look at the RMSE for the new fits

set.seed(1111) 

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5)


lmfit2 <- train(lat~ ., data = data_train, method = "lm", preProc = c("center", "scale"), trControl = fitControl) 



print(lmfit2)



```

```{r}
set.seed(1111) 



fitControl <- trainControl(method="repeatedcv",number=5,repeats=5)



earthfit2 <- train(lat~ ., data = data_train, method = "earth", preProc = c("center", "scale"), trControl = fitControl) 



print(earthfit2)
```

```{r}
set.seed(1111) #Preprocessed knn fit



fitControl <- trainControl(method="repeatedcv",number=5,repeats=5)



knnfit2 <- train(lat~ ., data = data_train, method = "knn", preProc = c("center", "scale"), trControl = fitControl) 



print(knnfit2)



```

So it looks like the linear mode got a bit better, KNN actually got worse, and MARS didn't change much. Since for KNN, "the data is the model", removing some predictors might have had a detrimental impact. Though to say something more useful, I would want to look much closer into what's going on and if these pre-processing steps are useful or not. For this exercise, let's move on.

## Model uncertainty

We can look at the uncertainty in model performance, e.g., the RMSE. Let's look at it for the models fit to the un-processed data.

```{r uncertainty}
#Use the `resamples` function in caret to extract uncertainty from the 3 models fit to the data  that doesn't have predictor pre-processing, then plot it

resamples_nonprocessed <- resamples(list(knn_fit, lm_fit, earth_fit_1))



resamples_nonprocessed %>% ggplot()
```

It seems that the model uncertainty for the outcome is fairly narrow for all models. We can (and in a real setting should) do further explorations to decide which model to choose. This is based part on what the model results are, and part on what we want. If we want a very simple, interpretable model, we'd likely use the linear model. If we want a model that has better performance, we might use MARS or - with the un-processed dataset - KNN.


## Residual plots

For this exercise, let's just pick one model. We'll go with the best performing one, namely KNN (fit to non-pre-processed data). Let's take a look at the residual plot.

```{r}
data_train$knn_predict <- predict(knn_fit)



ggplot(data_train, aes(x = data_train$knn_predict, y = data_train$lat)) + geom_point() +xlab("Predicted") + ylab("Actual")
#Write code to get model predictions for the outcome on the training data, and plot it as function of actual outcome values.
#also compute residuals (the difference between prediction and actual outcome) and plot that

data_train$knn_residuals <- residuals(knn_fit)



ggplot(data_train, aes(x = data_train$knn_predict, y = data_train$knn_residuals)) + geom_point() + xlab("Predicted") + ylab("Residuals")

```

Both plots look ok, predicted vs. outcome is along the 45-degree line, and the residual plot shows no major pattern. Of course, for a real analysis, we would again want to dig a bit deeper. But we'll leave it at this for now. 


## Final model evaluation

Let's do a final check, evaluate the performance of our final model on the test set.

```{r}
#Write code that computes model predictions and for test data, then compute SSR and RMSE.

data_test$knn_predict <- predict(knn_fit, data_test)

ggplot(data_test, aes(x = data_test$knn_predict, y = data_test$lat)) + geom_point() +xlab("Predicted") + ylab("Actual")


test_residuals <- residuals(knn_fit, data_test)


RMSE(data_test$lat, data_test$knn_predict) 
#RMSE = 11.1628


sum(test_residuals^2) 





```